{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\debor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(inputString):\n",
    "  return bool(re.search(r'^[a-zA-Z]+$', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_space(old_string):\n",
    "  new_string = old_string.replace(\" \", \" _IS_SPACE_ \")\n",
    "  return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(inputString):\n",
    "  return bool(re.search(r'\\d', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(wordlist):\n",
    "  wordfreq = []\n",
    "  listw = wordlist.copy()\n",
    "  word_frequency = []\n",
    "  for w in listw:\n",
    "    frequency = listw.count(w)\n",
    "    wordfreq.append(frequency)\n",
    "    word_frequency.append((w[0], w[1], frequency))\n",
    "  return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_template(cluster_tagger, log):\n",
    "  new_log = replace_space(log)\n",
    "  tokens = wordpunct_tokenize(new_log)\n",
    "  variables_list = []\n",
    "  template = ''\n",
    "  for token in tokens:\n",
    "    if token != \"_IS_SPACE_\":\n",
    "      for word in cluster_tagger:\n",
    "        if word[0] == token:\n",
    "          info = word[3]\n",
    "          break\n",
    "      if info == 'variable':\n",
    "        variables_list.append(token)\n",
    "        template = template + '<*>'\n",
    "      else:\n",
    "        template = template + token\n",
    "    else:\n",
    "      template = template + ' '\n",
    "  return template, variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_position (wordlist):\n",
    "  word_position_list = []\n",
    "  position = 0\n",
    "  for word in wordlist:\n",
    "    word_position_list.append((word, position))\n",
    "    position = position + 1\n",
    "  return word_position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated(wordfrequency):\n",
    "  new_wordfrequency = []\n",
    "  for word in wordfrequency:\n",
    "    if word not in new_wordfrequency:\n",
    "      new_wordfrequency.append(word)\n",
    "  return new_wordfrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_classifier(wordfrequency):\n",
    "  frequency_list = list(map(operator.itemgetter(2), wordfrequency))\n",
    "  p30 = np.percentile(frequency_list, 30)\n",
    "  label = []\n",
    "  for word in wordfrequency:\n",
    "    if word[2] < p30 or has_numbers(word[0]):\n",
    "      # word[0] -> token\n",
    "      # word[1] -> posicao\n",
    "      # word[2] -> quantidade\n",
    "      label.append((word[0],word[1],word[2],\"variable\"))\n",
    "    else:\n",
    "      label. append((word[0],word[1],word[2],\"template\"))\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_accuracy(data):\n",
    "  log_per_template =  data['EventId'].value_counts().to_dict()\n",
    "  correct = 0\n",
    "  for cluster in np.unique(data['Cluster']):\n",
    "    data_cluster = data.loc[data['Cluster'] == cluster]\n",
    "    log_per_template_cluster =  data_cluster['EventId'].value_counts().to_dict()\n",
    "    for eventid in np.unique(data_cluster['EventId']):\n",
    "      if log_per_template[eventid] == log_per_template_cluster[eventid]:\n",
    "        correct = correct + log_per_template_cluster[eventid]\n",
    "  return correct/len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_accuracy(data):\n",
    "  log_per_template =  data['EventId'].value_counts().to_dict()\n",
    "  correct = 0\n",
    "  for cluster in np.unique(data['Cluster']):\n",
    "    data_cluster = data.loc[data['Cluster'] == cluster]\n",
    "    log_per_template_cluster =  data_cluster['EventId'].value_counts().to_dict()\n",
    "    for eventid in np.unique(data_cluster['EventId']):\n",
    "      if len(np.unique(data_cluster['EventId'])) == 1:\n",
    "        correct = correct + log_per_template_cluster[eventid]\n",
    "  return correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_cluster_accuracy(data):\n",
    "  log_per_template =  data['EventId'].value_counts().to_dict()\n",
    "  correct = 0\n",
    "  for cluster in np.unique(data['Cluster']):\n",
    "    data_cluster = data.loc[data['Cluster'] == cluster]\n",
    "    log_per_template_cluster =  data_cluster['EventId'].value_counts().to_dict()\n",
    "    for eventid in np.unique(data_cluster['EventId']):\n",
    "      if log_per_template[eventid] == log_per_template_cluster[eventid] and len(np.unique(data_cluster['EventId'])) == 1:\n",
    "        correct = correct + log_per_template_cluster[eventid]\n",
    "  return correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_evaluation(data):\n",
    "  resultado1 = parsing_accuracy(data)\n",
    "  resultado2 = cluster_accuracy(data)\n",
    "  resultado3 = parsing_cluster_accuracy(data)\n",
    "  resultado = (resultado1 + resultado2 + resultado3)/3\n",
    "  return resultado, resultado1, resultado2, resultado3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogScan:\n",
    "  def __init__(self, logdata: list, header: bool, header_regex = None):\n",
    "    print('- Logscan v1.0')\n",
    "    if header:\n",
    "      print('-- Header Extraction')\n",
    "      loglist= [re.sub(f'{header_regex}', '', log) for log in logdata]\n",
    "      self.data = pd.DataFrame(loglist,columns=['Log'])\n",
    "    else:\n",
    "      self.data = pd.DataFrame(logdata,columns=['Log'])\n",
    "\n",
    "  def clean_data(self):\n",
    "    print('-- Data Cleaning')\n",
    "    clear_content = []\n",
    "    for _, row in self.data.iterrows():\n",
    "        raw_log = row['Log']\n",
    "        log_tokens = regex_tokenizer.tokenize(raw_log)\n",
    "        clean_text = []\n",
    "        for token in log_tokens:\n",
    "            if is_word(token):\n",
    "                clean_text.append(token)\n",
    "        clean_log = ' '.join(clean_text)\n",
    "        clear_content.append(clean_log)\n",
    "    self.data['CleanLog'] = clear_content\n",
    "\n",
    "  def tfidf_transformer(self):\n",
    "    print('-- TF-IDF Transformer')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(self.data['CleanLog'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    logs_embedding_df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return logs_embedding_df\n",
    "\n",
    "  def dbscanModel(self, logs_embedding_df):\n",
    "    print('-- DBSCAN')\n",
    "    clusterModel = DBSCAN(min_samples=2)\n",
    "    clusterModel.fit(logs_embedding_df)\n",
    "    self.data['Cluster'] = clusterModel.labels_\n",
    "\n",
    "  def word_tagger(self):\n",
    "    print('-- Word Tagger')\n",
    "    tagger = []\n",
    "    for cluster in np.unique(self.data['Cluster']):\n",
    "      cluster_tokens = []\n",
    "      dados_cluster = self.data.loc[self.data['Cluster'] == cluster]['Log']\n",
    "      for log in dados_cluster:\n",
    "        tokens = wordpunct_tokenize(log)\n",
    "        tokens_position = word_position (tokens)\n",
    "        cluster_tokens = cluster_tokens + tokens_position\n",
    "      word_frequency = word_counter(cluster_tokens)\n",
    "      new_wordfrequency = remove_repeated(word_frequency)\n",
    "      wordlabel = word_classifier(new_wordfrequency)\n",
    "      tagger.append((cluster, wordlabel))\n",
    "    return tagger\n",
    "\n",
    "  def create_templates(self, tagger):\n",
    "    print('-- Template Extraction')\n",
    "    templates = []\n",
    "    variables = []\n",
    "    for index, row in self.data.iterrows():\n",
    "      log_cluster = row['Cluster']\n",
    "      for cluster_tagger in tagger:\n",
    "        if cluster_tagger[0] == log_cluster:\n",
    "          log_tagger = cluster_tagger[1]\n",
    "          break\n",
    "      template, variables_list = log_template(log_tagger, row['Log'])\n",
    "      templates.append(template)\n",
    "      variables.append(variables_list)\n",
    "    self.data['Template'] = templates\n",
    "    self.data['Variables'] = variables\n",
    "\n",
    "  def pipeline(self):\n",
    "    self.clean_data()\n",
    "    log_embedding_df = self.tfidf_transformer()\n",
    "    self.dbscanModel(log_embedding_df)\n",
    "    tagger = self.word_tagger()\n",
    "    self.create_templates(tagger)\n",
    "    return tagger, self.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com o dataset Android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Logscan v1.0\n",
      "-- Data Cleaning\n",
      "-- TF-IDF Transformer\n",
      "-- DBSCAN\n",
      "-- Word Tagger\n",
      "-- Template Extraction\n"
     ]
    }
   ],
   "source": [
    "android_dataset = pd.read_csv(\"../logs/Andriod_2k.log_structured.csv\")\n",
    "log_scan_android = LogScan(list(android_dataset['Content']), header=False)\n",
    "tagger_android, result_dataset = log_scan_android.pipeline()\n",
    "result_dataset.to_csv('resultados.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
